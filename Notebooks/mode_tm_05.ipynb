{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721aac32",
   "metadata": {},
   "source": [
    "**<h1 align=\"center\">Text Mining</h1>**\n",
    "**<h2 align=\"center\">Stock Sentiment: Predicting market behavior from tweets</h2>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec56ca83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "334289b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Text Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Model Evaluation & Utilities\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.utils import resample, class_weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Scikit-learn Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# PyTorch Core\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, Dataset\n",
    "\n",
    "# Transformers (Hugging Face)\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, DistilBertTokenizerFast, DistilBertModel,\n",
    "    DistilBertForSequenceClassification, BertTokenizer, BertModel,\n",
    "    Trainer, TrainingArguments, AutoTokenizer, AutoModel\n",
    ")\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Sentence Transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# TensorFlow / Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.losses import CategoricalFocalCrossentropy, SparseCategoricalCrossentropy\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e4b2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df_train = pd.read_csv('../Data/train.csv')\n",
    "df_test = pd.read_csv('../Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5cb584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df_train, test_size=0.2, stratify=df_train['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d23384d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"chapter4\"></a>\n",
    "\n",
    "# 4. Data Preprocessing\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab3868",
   "metadata": {},
   "source": [
    "To prepare the tweets for modeling, we applied a series of text preprocessing steps aligned with the theoretical guidelines from the course. The goal was to clean and normalize the text while retaining meaningful linguistic content for feature extraction.\n",
    "\n",
    "The following techniques were applied:\n",
    "\n",
    "`Lowercasing`  All text was converted to lowercase to ensure uniformity (e.g., “Stock” and “stock” are treated the same).\n",
    "\n",
    "`Noise Removal` We removed URLs, mentions, hashtags, and retweet markers (RT) using regular expressions, as these tokens do not contribute to sentiment analysis.\n",
    "\n",
    "`Punctuation and Digit Removal` All punctuation symbols and numeric characters were stripped to reduce sparsity and dimensionality.\n",
    "\n",
    "`Tokenization` We used NLTK’s TreebankWordTokenizer, a rule-based tokenizer that works efficiently without external dependencies like punkt.\n",
    "\n",
    "`Stopword Removal and Lemmatization` We filtered out common English stopwords (e.g., \"and\", \"the\", \"of\") and lemmatized each remaining token to reduce inflectional variations (e.g., “running” → “run”).\n",
    "\n",
    "These steps produced a clean and compact representation of the original tweets, ready for downstream tasks like vectorization and sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e7ec2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ruben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ruben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ruben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ruben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a167f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7f57ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Regex Cleaning\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", '', text)                         # Remove URLs\n",
    "    text = re.sub(r\"@\\w+|#\\w+|rt\", '', text)                           # Remove mentions, hashtags, RT\n",
    "    text = re.sub(r\"br\", \"\", text)                                     # Remove 'br' (e.g. <br> tags)\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)      # Remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", ' ', text)                           # Remove numbers and special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()                           # Remove extra whitespace\n",
    "\n",
    "    # 3. Tokenize using Treebank tokenizer\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # 4. Remove stopwords and short tokens, then lemmatize\n",
    "    clean_tokens = [\n",
    "        lemmatizer.lemmatize(token)\n",
    "        for token in tokens\n",
    "        if token not in stop_words and len(token) > 2\n",
    "    ]\n",
    "\n",
    "    return \" \".join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20760254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to train and validation datasets\n",
    "train_df['clean_text'] = train_df['text'].fillna('').apply(preprocess_text)\n",
    "val_df['clean_text']   = val_df['text'].fillna('').apply(preprocess_text)\n",
    "df_test['clean_text']  = df_test['text'].fillna('').apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8c0c7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:\n",
      " Could Applied DNA Sciences, Inc. (APDN) See a Reversal After Breaking Its 52 Week Low? - The Lamp News\n",
      "Cleaned tweet:\n",
      " could applied dna science inc apdn see reversal eaking week low lamp news\n"
     ]
    }
   ],
   "source": [
    "# Check before and after cleaning\n",
    "print(\"Original tweet:\\n\", train_df['text'].iloc[6])\n",
    "print(\"Cleaned tweet:\\n\", train_df['clean_text'].iloc[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a38bb",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"chapter5\"></a>\n",
    "\n",
    "# 5. Feature Engineering\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42838841",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean_text'] = train_df['text'].fillna('').apply(preprocess_text)\n",
    "val_df['clean_text']   = val_df['text'].fillna('').apply(preprocess_text)\n",
    "df_test['clean_text']  = df_test['text'].fillna('').apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1be9d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['label']\n",
    "y_val = val_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f510c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cleaned = train_df['clean_text']\n",
    "X_val_cleaned = val_df['clean_text']\n",
    "X_test_cleaned = df_test['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65a461eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'text', 'clean_text'], dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b3d819",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"sub-section-5_4\"></a>\n",
    "\n",
    "## 5.4. Distil BERT\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4739755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained DistilBERT model and tokenizer\n",
    "bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db5ea864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = bert_tokenizer(\n",
    "            texts.tolist(),\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=64\n",
    "        )\n",
    "        self.labels = labels.tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class InferenceDataset(Dataset): # for the test dataset\n",
    "    def __init__(self, texts):\n",
    "        self.encodings = bert_tokenizer(\n",
    "            texts.tolist(),\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=64\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = FinDataset(X_train_cleaned, y_train)\n",
    "val_dataset   = FinDataset(X_val_cleaned, y_val)\n",
    "test_dataset = InferenceDataset(X_test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea6fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0388fce8",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"sub-section-6_6\"></a>\n",
    "\n",
    "## 6.6. Transformer\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced2854",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"sub-section-6_6_1\"></a>\n",
    "\n",
    "### 6.6.1. Distil BERT Fine-tuned\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6adf4f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load tokenizer and model\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e23be2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining metrics\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    labels = pred.label_ids\n",
    "    return {\n",
    "        \"precision\": precision_score(labels, preds, average=\"macro\"),\n",
    "        \"recall\": recall_score(labels, preds, average=\"macro\"),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25a9ebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_output\",       \n",
    "    do_train=True, # Training mode\n",
    "    do_eval=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    save_strategy=\"no\",                   \n",
    "    logging_strategy=\"no\",              \n",
    "    report_to=[],                          \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a1fcd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='478' max='478' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [478/478 08:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=478, training_loss=0.5208550616787069, metrics={'train_runtime': 536.9231, 'train_samples_per_second': 28.436, 'train_steps_per_second': 0.89, 'total_flos': 201464773197336.0, 'train_loss': 0.5208550616787069, 'epoch': 2.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 6. Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aac130b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on the test dataset\n",
    "test_preds = trainer.predict(test_dataset)\n",
    "y_test_pred = np.argmax(test_preds.predictions, axis=1)\n",
    "\n",
    "# Save predictions to CSV\n",
    "df_test['prediction'] = y_test_pred\n",
    "df_test[['text', 'prediction']].to_csv(\"bert_test_predictions.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
