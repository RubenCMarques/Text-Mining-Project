{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721aac32",
   "metadata": {},
   "source": [
    "**<h1 align=\"center\">Text Mining</h1>**\n",
    "**<h2 align=\"center\">Stock Sentiment: Predicting market behavior from tweets</h2>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec56ca83",
   "metadata": {},
   "source": [
    "This notebook presents the final solution for our Text Mining project on market sentiment classification based on tweets. Our approach uses the DistilBERT transformer model, fine-tuned on the labeled training data to classify each tweet as Bearish (0), Bullish (1), or Neutral (2). The solution includes preprocessing, tokenization, dataset preparation, model training, evaluation, and prediction. For a detailed overview of the experimentation process and alternative methods tested, please refer to the tm_tests_05 notebook and the accompanying report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520befe7",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"chapter1\"></a>\n",
    "\n",
    "# 1. Imports\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334289b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\.conda\\envs\\DeepLearning2425\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ruben\\.conda\\envs\\DeepLearning2425\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standard Libraries\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Text Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# PyTorch Core\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Transformers (Hugging Face)\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, DistilBertTokenizerFast, DistilBertForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4b2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df_train = pd.read_csv('../Data/train.csv')\n",
    "df_test = pd.read_csv('../Data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77909f3f",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"chapter2\"></a>\n",
    "\n",
    "# 2. Data Split\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5cb584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold-out method with stratification\n",
    "train_df, val_df = train_test_split(df_train, test_size=0.2, stratify=df_train['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d23384d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"chapter3\"></a>\n",
    "\n",
    "# 3. Data Preprocessing\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e7ec2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ruben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ruben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ruben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ruben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a167f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f57ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Regex Cleaning\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", '', text)                         # Remove URLs\n",
    "    text = re.sub(r\"@\\w+|#\\w+|rt\", '', text)                           # Remove mentions, hashtags, RT\n",
    "    text = re.sub(r\"br\", \"\", text)                                     # Remove 'br' (e.g. <br> tags)\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)      # Remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", ' ', text)                           # Remove numbers and special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()                           # Remove extra whitespace\n",
    "\n",
    "    # 3. Tokenize using Treebank tokenizer\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # 4. Remove stopwords and short tokens, then lemmatize\n",
    "    clean_tokens = [\n",
    "        lemmatizer.lemmatize(token)\n",
    "        for token in tokens\n",
    "        if token not in stop_words and len(token) > 2\n",
    "    ]\n",
    "\n",
    "    return \" \".join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20760254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to train, validation and test datasets\n",
    "train_df['clean_text'] = train_df['text'].fillna('').apply(preprocess_text)\n",
    "val_df['clean_text']   = val_df['text'].fillna('').apply(preprocess_text)\n",
    "df_test['clean_text']  = df_test['text'].fillna('').apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c0c7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:\n",
      " Could Applied DNA Sciences, Inc. (APDN) See a Reversal After Breaking Its 52 Week Low? - The Lamp News\n",
      "Cleaned tweet:\n",
      " could applied dna science inc apdn see reversal eaking week low lamp news\n"
     ]
    }
   ],
   "source": [
    "# Check before and after cleaning\n",
    "print(\"Original tweet:\\n\", train_df['text'].iloc[6])\n",
    "print(\"Cleaned tweet:\\n\", train_df['clean_text'].iloc[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42838841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cleaned text to the datasets\n",
    "train_df['clean_text'] = train_df['text'].fillna('').apply(preprocess_text)\n",
    "val_df['clean_text']   = val_df['text'].fillna('').apply(preprocess_text)\n",
    "df_test['clean_text']  = df_test['text'].fillna('').apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1be9d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels for training and validation sets\n",
    "y_train = train_df['label']\n",
    "y_val = val_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f510c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cleaned text for training, validation, and test sets\n",
    "X_train_cleaned = train_df['clean_text']\n",
    "X_val_cleaned = val_df['clean_text']\n",
    "X_test_cleaned = df_test['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65a461eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'text', 'clean_text'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the columns of the test DataFrame\n",
    "df_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a38bb",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"chapter4\"></a>\n",
    "\n",
    "# 4. Feature Engineering: Distil BERT\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4739755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained DistilBERT model and tokenizer\n",
    "bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db5ea864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset for training/validation data\n",
    "class FinDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = bert_tokenizer(\n",
    "            texts.tolist(),\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=64\n",
    "        )\n",
    "        self.labels = labels.tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Custom PyTorch Dataset for inference (test set), no labels\n",
    "class InferenceDataset(Dataset): \n",
    "    def __init__(self, texts):\n",
    "        self.encodings = bert_tokenizer(\n",
    "            texts.tolist(),\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=64\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "# Instantiate datasets for training, validation, and test sets\n",
    "train_dataset = FinDataset(X_train_cleaned, y_train)\n",
    "val_dataset   = FinDataset(X_val_cleaned, y_val)\n",
    "test_dataset = InferenceDataset(X_test_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388fce8",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"chapter5\"></a>\n",
    "\n",
    "# 5. Transformer\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced2854",
   "metadata": {},
   "source": [
    "**Distil BERT Fine-tuned**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6adf4f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e23be2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    labels = pred.label_ids\n",
    "    return {\n",
    "        \"precision\": precision_score(labels, preds, average=\"macro\"),\n",
    "        \"recall\": recall_score(labels, preds, average=\"macro\"),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25a9ebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training configuration and hyperparameters for the DistilBERT model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_output\",       \n",
    "    do_train=True, # Training mode\n",
    "    do_eval=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    save_strategy=\"no\",                   \n",
    "    logging_strategy=\"no\",              \n",
    "    report_to=[],                          \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a1fcd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='478' max='478' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [478/478 08:19, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=478, training_loss=0.5034384946942828, metrics={'train_runtime': 500.4012, 'train_samples_per_second': 30.512, 'train_steps_per_second': 0.955, 'total_flos': 201464773197336.0, 'train_loss': 0.5034384946942828, 'epoch': 2.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c87f5",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"chapter6\"></a>\n",
    "\n",
    "# 6. Predictions on Test\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aac130b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on the test dataset\n",
    "test_preds = trainer.predict(test_dataset)\n",
    "y_test_pred = np.argmax(test_preds.predictions, axis=1)\n",
    "\n",
    "# Save predictions to CSV\n",
    "df_test['prediction'] = y_test_pred\n",
    "df_test[['id', 'prediction']].to_csv(\"pred_05.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
